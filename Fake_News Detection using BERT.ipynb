{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08a2c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pycaret\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b66d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5097a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a3b1143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96eb2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68d1790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARITOSH MISHRA\\AppData\\Local\\Temp\\ipykernel_13288\\3094905971.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WOW! WIGGLE ROOM: Great Ad Hammers Clinton On ...</td>\n",
       "      <td>This ad by the Republican Party nails Hillary ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Feb 21, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump Fan Calls For Lynching Hillary, Screams...</td>\n",
       "      <td>As a shock to no one ever, the rally in Raleig...</td>\n",
       "      <td>News</td>\n",
       "      <td>July 6, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.N. chief says no alternative to two state so...</td>\n",
       "      <td>UNITED NATIONS (Reuters) - United Nations Secr...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 6, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sportsmen fight Trump energy plans alongside e...</td>\n",
       "      <td>WASHINGTON/SALT LAKE CITY, Utah (Reuters) - Wh...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 17, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russian aerobatics team joins Serbian display ...</td>\n",
       "      <td>BELGRADE (Reuters) - Russia s elite aerobatic ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 20, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  WOW! WIGGLE ROOM: Great Ad Hammers Clinton On ...   \n",
       "1   Trump Fan Calls For Lynching Hillary, Screams...   \n",
       "2  U.N. chief says no alternative to two state so...   \n",
       "3  Sportsmen fight Trump energy plans alongside e...   \n",
       "4  Russian aerobatics team joins Serbian display ...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  This ad by the Republican Party nails Hillary ...      politics   \n",
       "1  As a shock to no one ever, the rally in Raleig...          News   \n",
       "2  UNITED NATIONS (Reuters) - United Nations Secr...     worldnews   \n",
       "3  WASHINGTON/SALT LAKE CITY, Utah (Reuters) - Wh...  politicsNews   \n",
       "4  BELGRADE (Reuters) - Russia s elite aerobatic ...     worldnews   \n",
       "\n",
       "                 date Target  \n",
       "0        Feb 21, 2016   Fake  \n",
       "1        July 6, 2016   Fake  \n",
       "2   December 6, 2017    True  \n",
       "3  February 17, 2017    True  \n",
       "4   October 20, 2017    True  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "true_data = pd.read_csv('a1_True.csv')\n",
    "fake_data = pd.read_csv('a2_Fake.csv')\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True']*len(true_data)\n",
    "fake_data['Target'] = ['Fake']*len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n",
    "\n",
    "# See how the data looks like\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77dffd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=1) \n",
    "data['label'] = pd.get_dummies(data.Target)['Fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a35647c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WOW! WIGGLE ROOM: Great Ad Hammers Clinton On ...</td>\n",
       "      <td>This ad by the Republican Party nails Hillary ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Feb 21, 2016</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump Fan Calls For Lynching Hillary, Screams...</td>\n",
       "      <td>As a shock to no one ever, the rally in Raleig...</td>\n",
       "      <td>News</td>\n",
       "      <td>July 6, 2016</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.N. chief says no alternative to two state so...</td>\n",
       "      <td>UNITED NATIONS (Reuters) - United Nations Secr...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 6, 2017</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sportsmen fight Trump energy plans alongside e...</td>\n",
       "      <td>WASHINGTON/SALT LAKE CITY, Utah (Reuters) - Wh...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 17, 2017</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russian aerobatics team joins Serbian display ...</td>\n",
       "      <td>BELGRADE (Reuters) - Russia s elite aerobatic ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 20, 2017</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  WOW! WIGGLE ROOM: Great Ad Hammers Clinton On ...   \n",
       "1   Trump Fan Calls For Lynching Hillary, Screams...   \n",
       "2  U.N. chief says no alternative to two state so...   \n",
       "3  Sportsmen fight Trump energy plans alongside e...   \n",
       "4  Russian aerobatics team joins Serbian display ...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  This ad by the Republican Party nails Hillary ...      politics   \n",
       "1  As a shock to no one ever, the rally in Raleig...          News   \n",
       "2  UNITED NATIONS (Reuters) - United Nations Secr...     worldnews   \n",
       "3  WASHINGTON/SALT LAKE CITY, Utah (Reuters) - Wh...  politicsNews   \n",
       "4  BELGRADE (Reuters) - Russia s elite aerobatic ...     worldnews   \n",
       "\n",
       "                 date Target  label  \n",
       "0        Feb 21, 2016   Fake      1  \n",
       "1        July 6, 2016   Fake      1  \n",
       "2   December 6, 2017    True      0  \n",
       "3  February 17, 2017    True      0  \n",
       "4   October 20, 2017    True      0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e367c508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x1e89b3f6d60>,\n",
       "  <matplotlib.patches.Wedge at 0x1e89b416280>],\n",
       " [Text(-1.1968727067385088, -0.0865778485782335, 'Fake'),\n",
       "  Text(1.1968726986325005, 0.08657796063754254, 'True')],\n",
       " [Text(-0.6981757455974634, -0.05050374500396954, '52.3%'),\n",
       "  Text(0.6981757408689586, 0.05050381037189981, '47.7%')])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking if our data is well balanced\n",
    "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
    "plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ccd6f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test set split into 70:15:15 ratio\n",
    "# Train-Temp split\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=data['Target'])\n",
    "# Validation-Test split\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aae80f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model and tokenizer via HuggingFace Transformers\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83eddb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of texts')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of the number of words in train data 'title'\n",
    "seq_len = [len(title.split()) for title in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ac48cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 3857, 8275, 2739, 2944, 1012, 102], [101, 2478, 14324, 1012, 102, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# BERT Tokeizer Functionality\n",
    "sample_data = [\"Build fake news model.\", \n",
    "               \"Using bert.\"]                                         # sample data\n",
    "tokenized_sample_data = tokenizer.batch_encode_plus(sample_data,\n",
    "                                                    padding=True)     # encode text\n",
    "print(tokenized_sample_data)\n",
    "\n",
    "# Ref: https://huggingface.co/docs/transformers/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af1d4dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\testing\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Majority of titles above have word length under 15. So, we set max title length as 15\n",
    "MAX_LENGHT = 15\n",
    "# Tokenize and encode sequences in the train set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56f867da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18633be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader structure definition\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32                                               #define a batch size\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)    # wrap tensors\n",
    "train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "                                                              # dataLoader for train set\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)            # wrap tensors\n",
    "val_sampler = SequentialSampler(val_data)                     # sampler for sampling the data during training\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "                                                              # dataLoader for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81b3680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the parameters and defining trainable BERT structure\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False    # false here means gradient need not be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2afeccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):  \n",
    "      super(BERT_Arch, self).__init__()\n",
    "      self.bert = bert   \n",
    "      self.dropout = nn.Dropout(0.1)            # dropout layer\n",
    "      self.relu =  nn.ReLU()                    # relu activation function\n",
    "      self.fc1 = nn.Linear(768,512)             # dense layer 1\n",
    "      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function\n",
    "    def forward(self, sent_id, mask):           # define the forward pass  \n",
    "      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "                                                # pass the inputs to the model\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)                           # output layer\n",
    "      x = self.softmax(x)                       # apply softmax activation\n",
    "      return x\n",
    "\n",
    "model = BERT_Arch(bert)\n",
    "# Defining the hyperparameters (optimizer, weights of the classes and the epochs)\n",
    "# Define the optimizer\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate\n",
    "# Define the loss function\n",
    "cross_entropy  = nn.NLLLoss() \n",
    "# Number of training epochs\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0037520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining training and evaluation functions\n",
    "def train():  \n",
    "  model.train()\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  for step,batch in enumerate(train_dataloader):                # iterate over batches\n",
    "    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "    batch = [r for r in batch]                                  # push the batch to gpu\n",
    "    sent_id, mask, labels = batch \n",
    "    model.zero_grad()                                           # clear previously calculated gradients\n",
    "    preds = model(sent_id, mask)                                # get model predictions for current batch\n",
    "    loss = cross_entropy(preds, labels)                         # compute loss between actual & predicted values\n",
    "    total_loss = total_loss + loss.item()                       # add on to the total loss\n",
    "    loss.backward()                                             # backward pass to calculate the gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)     # clip gradients to 1.0. It helps in preventing exploding gradient problem\n",
    "    optimizer.step()                                            # update parameters\n",
    "    preds=preds.detach().cpu().numpy()                          # model predictions are stored on GPU. So, push it to CPU\n",
    "\n",
    "  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch  \n",
    "                                                                # reshape predictions in form of (# samples, # classes)\n",
    "  return avg_loss                                 # returns the loss and predictions\n",
    "\n",
    "def evaluate():  \n",
    "  print(\"\\nEvaluating...\")  \n",
    "  model.eval()                                    # Deactivate dropout layers\n",
    "  total_loss, total_accuracy = 0, 0  \n",
    "  for step,batch in enumerate(val_dataloader):    # Iterate over batches  \n",
    "    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.     \n",
    "                                                  # Calculate elapsed time in minutes.\n",
    "                                                  # Elapsed = format_time(time.time() - t0)\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "                                                  # Report progress\n",
    "    batch = [t for t in batch]                    # Push the batch to GPU\n",
    "    sent_id, mask, labels = batch\n",
    "    with torch.no_grad():                         # Deactivate autograd\n",
    "      preds = model(sent_id, mask)                # Model predictions\n",
    "      loss = cross_entropy(preds,labels)          # Compute the validation loss between actual and predicted values\n",
    "      total_loss = total_loss + loss.item()\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "  avg_loss = total_loss / len(val_dataloader)         # compute the validation loss of the epoch\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae1d0b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "  Batch    50  of    983.\n",
      "  Batch   100  of    983.\n",
      "  Batch   150  of    983.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\2650447131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Epoch {:} / {:}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                    \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_valid_loss\u001b[0m\u001b[1;33m:\u001b[0m              \u001b[1;31m# save the best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\2899621174.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                                           \u001b[1;31m# clear previously calculated gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m                                \u001b[1;31m# get model predictions for current batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m                         \u001b[1;31m# compute loss between actual & predicted values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# add on to the total loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\361266427.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sent_id, mask)\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# softmax activation function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m           \u001b[1;31m# define the forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m       \u001b[0mcls_hs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pooler_output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                                                 \u001b[1;31m# pass the inputs to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         )\n\u001b[1;32m-> 1014\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1015\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m                 )\n\u001b[0;32m    602\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    604\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         )\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add attentions if we output them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\testing\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and predict\n",
    "best_valid_loss = float('inf')\n",
    "train_losses=[]                   # empty lists to store training and validation loss of each epoch\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))     \n",
    "    train_loss = train()                       # train model\n",
    "    valid_loss = evaluate()                    # evaluate model\n",
    "    if valid_loss < best_valid_loss:              # save the best model\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'c2_new_model_weights_v2.pt')\n",
    "    train_losses.append(train_loss)               # append training and validation loss\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29b7a12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights of best model\n",
    "path = 'c2_new_model_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fef12f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85      3212\n",
      "           1       0.91      0.78      0.84      3523\n",
      "\n",
      "    accuracy                           0.84      6735\n",
      "   macro avg       0.85      0.85      0.84      6735\n",
      "weighted avg       0.85      0.84      0.84      6735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  preds = model(test_seq, test_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71fe7858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446919079435783"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "accuracy_score(test_y,preds)\n",
    "confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e6f6f",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df921cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model and tokenizer via HuggingFace Transformers\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):  \n",
    "      super(BERT_Arch, self).__init__()\n",
    "      self.bert = bert   \n",
    "      self.dropout = nn.Dropout(0.1)            # dropout layer\n",
    "      self.relu =  nn.ReLU()                    # relu activation function\n",
    "      self.fc1 = nn.Linear(768,512)             # dense layer 1\n",
    "      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function\n",
    "    def forward(self, sent_id, mask):           # define the forward pass  \n",
    "      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "                                                # pass the inputs to the model\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)                           # output layer\n",
    "      x = self.softmax(x)                       # apply softmax activation\n",
    "      return x\n",
    "\n",
    "n_model = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1b9518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager-core).\n",
      "Your token has been saved to C:\\Users\\PARITOSH MISHRA\\.huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4afa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a9a971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"Parit0sh/FakeNews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c4e036e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # load weights of best model\n",
    "path = 'c2_new_model_weights.pt'\n",
    "n_model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a92afbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing on unseen data\n",
    "unseen_news_text = [\"Donald Trump Sends Out Embarrassing New Years Eve Message; This is Disturbing\",     # Fake\n",
    "                    \"WATCH: George W. Bush Calls Out Trump For Supporting White Supremacy\",               # Fake\n",
    "                    \"U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources\",           # True\n",
    "                    \"Trump administration issues new rules on U.S. visa waivers\"                          # True\n",
    "                    ]\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "MAX_LENGHT = 15\n",
    "tokens_unseen = tokenizer.batch_encode_plus(\n",
    "    unseen_news_text,\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "unseen_seq = torch.tensor(tokens_unseen['input_ids'])\n",
    "unseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = n_model(unseen_seq, unseen_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4e3c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources\n",
      "Legit\n"
     ]
    }
   ],
   "source": [
    "text = [input()]\n",
    "\n",
    "MAX_LENGHT = 15\n",
    "tokens_unseen = tokenizer.batch_encode_plus(\n",
    "    text,\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "unseen_seq = torch.tensor(tokens_unseen['input_ids'])\n",
    "unseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = n_model(unseen_seq, unseen_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(\"Fake\" if preds == 1 else \"Legit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0bafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5571c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
